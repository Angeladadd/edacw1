- name: Run Analysis Pipeline
  hosts: hostnode
  vars:
    ppl_path: "/home/almalinux/pipeline"
    script_args: "ecoli --partitions 300"
  tasks:
  - name: Run spark-submit
    shell: |
        spark-submit \
        --master yarn \
        --deploy-mode cluster \
        --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
        --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON={{ ppl_path }}/venv/bin/python \
        --conf spark.yarn.appMasterEnv.PYTHONPATH={{ ppl_path }}/venv/lib/python3.9/site-packages:$PYTHONPATH \
        --conf spark.executorEnv.MPLCONFIGDIR=/tmp/matplotlib \
        --conf spark.executorEnv.PYSPARK_PYTHON={{ ppl_path }}/venv/bin/python \
        --conf spark.executorEnv.PYTHONPATH={{ ppl_path }}/venv/lib/python3.9/site-packages:$PYTHONPATH \
        --py-files {{ ppl_path }}/utils.zip \
        {{ ppl_path }}/pipeline.py {{ script_args }} 2>&1
    register: spark_submit_output
  - name: Extract application ID
    set_fact:
      spark_application_id: "{{ spark_submit_output.stdout | regex_search('application_[0-9_]+') }}"
  - name: Print application ID
    debug:
      msg: |
        Spark Application UI: {{ yarn_url }}/proxy/{{ spark_application_id }}